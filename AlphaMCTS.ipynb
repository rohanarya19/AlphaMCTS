{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "\n",
    "config_map = {\n",
    "    'device': torch.device('cuda') if torch.cuda.is_available() else 'cpu',\n",
    "    'num_filters': 128,              # Number of convolutional filters used in residual blocks\n",
    "    'num_residual_blocks': 8,             # Number of residual blocks used in network\n",
    "    'exploration_constant': 2,     # Exploration constant used in PUCT calculation\n",
    "    'selection_temperature': 1.25,           # Selection temperature. A greater temperature is a more uniform distribution\n",
    "    'dirichlet_alpha': 1.,         # Alpha parameter for Dirichlet noise. Larger values mean more uniform noise\n",
    "    'dirichlet_epsilon': 0.25,         # Weight of dirichlet noise\n",
    "    'learning_rate': 0.001,        # Adam learning rate\n",
    "    'training_epochs': 1,         # How many full training epochs\n",
    "    'games_per_epoch': 1,        # How many self-played games per epoch\n",
    "    'minibatch_size': 128,         # Size of each minibatch used in learning update \n",
    "    'num_minibatches': 4,            # How many minibatches to accumulate per learning step \n",
    "    'mcts_initial_iterations': 50,  # Number of Monte Carlo tree search iterations initially\n",
    "    'mcts_max_iterations': 150,   # Maximum number of MCTS iterations\n",
    "    'mcts_search_increment': 1,    # After each epoch, how much should search iterations be increased by\n",
    "}\n",
    "\n",
    "# Convert to a struct-esque object\n",
    "class Config:\n",
    "    def __init__(self, mapping):\n",
    "        for key, value in mapping.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "config = Config(config_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4Engine:\n",
    "    \"Engine for Connect 4 game with methods for game-related tasks.\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_rows = 6\n",
    "        self.num_cols = 7\n",
    "\n",
    "    def get_next_state(self, current_state, selected_action, to_play=1):\n",
    "        \"Update the current state based on the selected action and return the resulting board.\"\n",
    "        # Preconditions\n",
    "        assert self.evaluate(current_state) == 0\n",
    "        assert np.sum(abs(current_state)) != self.num_rows * self.num_cols\n",
    "        assert selected_action in self.get_valid_actions(current_state)\n",
    "        \n",
    "        # Find the next empty row in the selected column\n",
    "        row_index = np.where(current_state[:, selected_action] == 0)[0][-1]\n",
    "        \n",
    "        # Apply the action\n",
    "        new_state = current_state.copy()\n",
    "        new_state[row_index, selected_action] = to_play\n",
    "        return new_state\n",
    "\n",
    "    def get_valid_actions(self, current_state):\n",
    "        \"Return an array containing the indices of valid actions.\"\n",
    "        # If the game is over, there are no valid moves\n",
    "        if self.evaluate(current_state) != 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Identify the columns where pieces can be placed\n",
    "        col_sums = np.sum(np.abs(current_state), axis=0)\n",
    "        return np.where((col_sums // self.num_rows) == 0)[0]\n",
    "\n",
    "    def evaluate(self, current_state):\n",
    "        \"Evaluate the current position. Returns 1 for player 1 win, -1 for player 2, and 0 otherwise.\"\n",
    "        # Kernels for checking win conditions\n",
    "        kernel = np.ones((1, 4), dtype=int)\n",
    "        \n",
    "        # Horizontal and vertical checks\n",
    "        horizontal_check = convolve2d(current_state, kernel, mode='valid')\n",
    "        vertical_check = convolve2d(current_state, kernel.T, mode='valid')\n",
    "\n",
    "        # Diagonal checks\n",
    "        diagonal_kernel = np.eye(4, dtype=int)\n",
    "        main_diagonal_check = convolve2d(current_state, diagonal_kernel, mode='valid')\n",
    "        anti_diagonal_check = convolve2d(current_state, np.fliplr(diagonal_kernel), mode='valid')\n",
    "        \n",
    "        # Check for a winner\n",
    "        if any(condition.any() for condition in [horizontal_check == 4, vertical_check == 4, main_diagonal_check == 4, anti_diagonal_check == 4]):\n",
    "            return 1\n",
    "        elif any(condition.any() for condition in [horizontal_check == -4, vertical_check == -4, main_diagonal_check == -4, anti_diagonal_check == -4]):\n",
    "            return -1\n",
    "\n",
    "        # No winner\n",
    "        return 0  \n",
    "\n",
    "    def play(self, current_state, selected_action, to_play=1):\n",
    "        \"Execute an action in the current state. Return the next state, reward, and termination flag.\"\n",
    "        # Obtain the new state and reward\n",
    "        next_state = self.get_next_state(current_state, selected_action, to_play)\n",
    "        reward = self.evaluate(next_state)\n",
    "        \n",
    "        # Check if the game has ended\n",
    "        game_over = True if reward != 0 or np.sum(abs(next_state)) >= (self.num_rows * self.num_cols - 1) else False\n",
    "        return next_state, reward, game_over\n",
    "\n",
    "    def encode_state(self, current_state):\n",
    "        \"Convert the state to a tensor with 3 channels.\"\n",
    "        encoded_state = np.stack((current_state == 1, current_state == 0, current_state == -1)).astype(np.float32)\n",
    "        if len(current_state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        return encoded_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the game board.\"\n",
    "        return np.zeros([self.num_rows, self.num_cols], dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNeuralNetwork(nn.Module):\n",
    "    \"Complete residual neural network model.\"\n",
    "    \n",
    "    def __init__(self, game_engine, model_config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Board dimensions\n",
    "        self.board_size = (game_engine.num_rows, game_engine.num_cols)\n",
    "        num_actions = game_engine.num_cols  # Number of columns represent possible actions\n",
    "        num_filters = model_config.num_filters\n",
    "        \n",
    "        self.base = ConvolutionBase(model_config)  # Base layers\n",
    "\n",
    "        # Policy head for action selection\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters//4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_filters//4 * self.board_size[0] * self.board_size[1], num_actions)\n",
    "        )\n",
    "\n",
    "        # Value head for state evaluation\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters//32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters//32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_filters//32 * self.board_size[0] * self.board_size[1], 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x) \n",
    "        x_value = self.value_head(x)\n",
    "        x_policy = self.policy_head(x)\n",
    "        return x_value, x_policy\n",
    "\n",
    "class ConvolutionBase(nn.Module):\n",
    "    \"Convolutional base for the network.\"\n",
    "    \n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        num_filters = model_config.num_filters\n",
    "        num_residual_blocks = model_config.num_residual_blocks\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # List of residual blocks\n",
    "        self.res_blocks = nn.ModuleList(\n",
    "            [ResidualBlock(num_filters) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"Residual block, the backbone of a ResNet.\"\n",
    "    \n",
    "    def __init__(self, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # Two convolutional layers, both with batch normalization\n",
    "        self.conv_1 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(num_filters)\n",
    "        self.conv_2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(num_filters)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass x through layers and add skip connection\n",
    "        output = self.relu(self.batch_norm_1(self.conv_1(x)))\n",
    "        output = self.batch_norm_2(self.conv_2(output))\n",
    "        return self.relu(output + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearch:\n",
    "    def __init__(self, neural_net, game_engine, model_config):\n",
    "        \"Initialize Monte Carlo Tree Search with a given neural network, game instance, and configuration.\"\n",
    "        self.neural_net = neural_net\n",
    "        self.game_engine = game_engine\n",
    "        self.model_config = model_config\n",
    "\n",
    "    def search(self, initial_state, total_iterations, selection_temperature=None):\n",
    "        \"Performs a search for the desired number of iterations, returns an action and the tree root.\"\n",
    "        # Create the root\n",
    "        root = TreeNode(None, initial_state, 1, self.game_engine, self.model_config)\n",
    "\n",
    "        # Expand the root, adding noise to each action\n",
    "        valid_actions = self.game_engine.get_valid_actions(initial_state)\n",
    "        state_tensor = torch.tensor(self.game_engine.encode_state(initial_state), dtype=torch.float).unsqueeze(0).to(self.model_config.device)\n",
    "        with torch.no_grad():\n",
    "            self.neural_net.eval()\n",
    "            value, logits = self.neural_net(state_tensor)\n",
    "\n",
    "        # Get action probabilities\n",
    "        action_probs = F.softmax(logits.view(self.game_engine.num_cols), dim=0).cpu().numpy()\n",
    "\n",
    "        # Calculate and add Dirichlet noise\n",
    "        noise = np.random.dirichlet([self.model_config.dirichlet_alpha] * self.game_engine.num_cols)\n",
    "        action_probs = ((1 - self.model_config.dirichlet_epsilon) * action_probs) + self.model_config.dirichlet_epsilon * noise\n",
    "\n",
    "        # Mask unavailable actions\n",
    "        mask = np.full(self.game_engine.num_cols, False)\n",
    "        mask[valid_actions] = True\n",
    "        action_probs = action_probs[mask]\n",
    "\n",
    "        # Softmax\n",
    "        action_probs /= np.sum(action_probs)\n",
    "\n",
    "        # Create a child for each possible action\n",
    "        for action, prob in zip(valid_actions, action_probs):\n",
    "            child_state = -self.game_engine.get_next_state(initial_state, action)\n",
    "            root.children[action] = TreeNode(root, child_state, -1, self.game_engine, self.model_config)\n",
    "            root.children[action].prob = prob\n",
    "\n",
    "        # Since we're not backpropagating, manually increase visits\n",
    "        root.n_visits = 1\n",
    "        # Set value as neural network prediction also as it will slightly improve the accuracy of the value target later\n",
    "        root.total_score = value.item()\n",
    "\n",
    "        # Begin search\n",
    "        for _ in range(total_iterations):\n",
    "            current_node = root\n",
    "\n",
    "            # Phase 1: Selection\n",
    "            # While not currently on a leaf node, select a new node using PUCT score\n",
    "            while not current_node.is_leaf():\n",
    "                current_node = current_node.select_child()\n",
    "\n",
    "            # Phase 2: Expansion\n",
    "            # When a leaf node is reached and it's not terminal; expand it\n",
    "            if not current_node.is_terminal():\n",
    "                current_node.expand()\n",
    "                # Convert node state to tensor and pass through network\n",
    "                state_tensor = torch.tensor(self.game_engine.encode_state(current_node.state), dtype=torch.float).unsqueeze(0).to(self.model_config.device)\n",
    "                with torch.no_grad():\n",
    "                    self.neural_net.eval()\n",
    "                    value, logits = self.neural_net(state_tensor)\n",
    "                    value = value.item()\n",
    "\n",
    "                # Mask invalid actions, then calculate masked action probs\n",
    "                mask = np.full(self.game_engine.num_cols, False)\n",
    "                mask[valid_actions] = True\n",
    "                action_probs = F.softmax(logits.view(self.game_engine.num_cols)[mask], dim=0).cpu().numpy()\n",
    "                for child, prob in zip(current_node.children.values(), action_probs):\n",
    "                    child.prob = prob\n",
    "            # If node is terminal, get the value of it from game instance\n",
    "            else:\n",
    "                value = self.game_engine.evaluate(current_node.state)\n",
    "\n",
    "            # Phase 3: Backpropagation\n",
    "            # Backpropagate the value of the leaf to the root\n",
    "            current_node.backpropagate(value)\n",
    "        \n",
    "        # Select action with specified selection_temperature\n",
    "        if selection_temperature == None:\n",
    "            selection_temperature = self.model_config.selection_temperature\n",
    "        return self.select_action(root, selection_temperature), root\n",
    "\n",
    "    def select_action(self, root, selection_temperature=None):\n",
    "        \"Select an action from the root based on visit counts, adjusted by selection_temperature, 0 temp for greedy.\"\n",
    "        if selection_temperature == None:\n",
    "            selection_temperature = self.model_config.selection_temperature\n",
    "        action_counts = {key: val.n_visits for key, val in root.children.items()}\n",
    "        if selection_temperature == 0:\n",
    "            return max(action_counts, key=action_counts.get)\n",
    "        elif selection_temperature == np.inf:\n",
    "            return np.random.choice(list(action_counts.keys()))\n",
    "        else:\n",
    "            distribution = np.array([*action_counts.values()]) ** (1 / selection_temperature)\n",
    "            return np.random.choice([*action_counts.keys()], p=distribution/sum(distribution))\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, parent, state, to_play, game_engine, model_config):\n",
    "        \"Represents a node in the MCTS, holding the game state and statistics for MCTS to operate.\"\n",
    "        self.parent = parent\n",
    "        self.state = state\n",
    "        self.to_play = to_play\n",
    "        self.model_config = model_config\n",
    "        self.game_engine = game_engine\n",
    "\n",
    "        self.prob = 0\n",
    "        self.children = {}\n",
    "        self.n_visits = 0\n",
    "        self.total_score = 0\n",
    "\n",
    "    def expand(self):\n",
    "        \"Create child nodes for all valid actions. If state is terminal, evaluate and set the node's value.\"\n",
    "        # Get valid actions\n",
    "        valid_actions = self.game_engine.get_valid_actions(self.state)\n",
    "\n",
    "        # If there are no valid actions, state is terminal, so get value using game instance\n",
    "        if len(valid_actions) == 0:\n",
    "            self.total_score = self.game_engine.evaluate(self.state)\n",
    "            return\n",
    "\n",
    "        # Create a child for each possible action\n",
    "        for action in valid_actions:\n",
    "            # Make move, then flip board to perspective of next player\n",
    "            child_state = -self.game_engine.get_next_state(self.state, action)\n",
    "            self.children[action] = TreeNode(self, child_state, -self.to_play, self.game_engine, self.model_config)\n",
    "\n",
    "    def select_child(self):\n",
    "        \"Select the child node with the highest PUCT score.\"\n",
    "        best_puct = -np.inf\n",
    "        best_child = None\n",
    "        for child in self.children.values():\n",
    "            puct = self.calculate_puct(child)\n",
    "            if puct > best_puct:\n",
    "                best_puct = puct\n",
    "                best_child = child\n",
    "        return best_child\n",
    "\n",
    "    def calculate_puct(self, child):\n",
    "        \"Calculate the PUCT score for a given child node.\"\n",
    "        # Scale Q(s,a) so it's between 0 and 1 so it's comparable to a probability\n",
    "        # Using 1 - Q(s,a) because it's from the perspective of the child – the opposite of the parent\n",
    "        exploitation_term = 1 - (child.get_value() + 1) / 2\n",
    "        exploration_term = child.prob * math.sqrt(self.n_visits) / (child.n_visits + 1)\n",
    "        return exploitation_term + self.model_config.exploration_constant * exploration_term\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        \"Update the current node and its ancestors with the given value.\"\n",
    "        self.total_score += value\n",
    "        self.n_visits += 1\n",
    "        if self.parent is not None:\n",
    "            # Backpropagate the negative value so it switches each level\n",
    "            self.parent.backpropagate(-value)\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"Check if the node is a leaf (no children).\"\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def is_terminal(self):\n",
    "        \"Check if the node represents a terminal state.\"\n",
    "        return (self.n_visits != 0) and (len(self.children) == 0)\n",
    "\n",
    "    def get_value(self):\n",
    "        \"Calculate the average value of this node.\"\n",
    "        if self.n_visits == 0:\n",
    "            return 0\n",
    "        return self.total_score / self.n_visits\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"Return a string containing the node's relevant information for debugging purposes.\"\n",
    "        return (f\"State:\\n{self.state}\\nProb: {self.prob}\\nTo play: {self.to_play}\" +\n",
    "                f\"\\nNumber of children: {len(self.children)}\\nNumber of visits: {self.n_visits}\" +\n",
    "                f\"\\nTotal score: {self.total_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaMCTS:\n",
    "    def __init__(self, game_engine, model_config, verbose=True):\n",
    "        self.network = ResidualNeuralNetwork(game_engine, model_config).to(model_config.device)\n",
    "        self.mcts = MonteCarloTreeSearch(self.network, game_engine, model_config)\n",
    "        self.game_engine = game_engine\n",
    "        self.model_config = model_config\n",
    "\n",
    "        # Losses and optimizer\n",
    "        self.loss_cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=model_config.learning_rate, weight_decay=0.0001)\n",
    "\n",
    "        # Pre-allocate memory on GPU\n",
    "        state_shape = game_engine.encode_state(game_engine.reset()).shape\n",
    "        self.max_memory = model_config.minibatch_size * model_config.num_minibatches\n",
    "        self.state_memory = torch.zeros(self.max_memory, *state_shape).to(model_config.device)\n",
    "        self.value_memory = torch.zeros(self.max_memory, 1).to(model_config.device)\n",
    "        self.policy_memory = torch.zeros(self.max_memory, game_engine.num_cols).to(model_config.device)\n",
    "        self.current_memory_index = 0\n",
    "        self.memory_full = False\n",
    "\n",
    "        # MCTS search iterations\n",
    "        self.search_iterations = model_config.mcts_initial_iterations\n",
    "        \n",
    "        # Logging\n",
    "        self.verbose = verbose\n",
    "        self.total_games = 0\n",
    "\n",
    "    def train(self, training_epochs):\n",
    "        \"Train the AlphaMCTS agent for a specified number of training epochs.\"\n",
    "        # For each training epoch\n",
    "        for _ in range(training_epochs):\n",
    "\n",
    "            # Play specified number of games\n",
    "            for _ in range(self.model_config.games_per_epoch):\n",
    "                self.self_play()\n",
    "            \n",
    "            # At the end of each epoch, increase the number of MCTS search iterations\n",
    "            self.search_iterations = min(self.model_config.mcts_max_iterations, self.search_iterations + self.model_config.mcts_search_increment)\n",
    "\n",
    "    def self_play(self):\n",
    "        \"Perform one episode of self-play.\"\n",
    "        state = self.game_engine.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Search for a move\n",
    "            action, root = self.mcts.search(state, self.search_iterations)\n",
    "\n",
    "            # Value target is the value of the MCTS root node\n",
    "            value = root.get_value()\n",
    "\n",
    "            # Visit counts used to compute policy target\n",
    "            visits = np.zeros(self.game_engine.num_cols)\n",
    "            for child_action, child in root.children.items():\n",
    "                visits[child_action] = child.n_visits\n",
    "            # Softmax so distribution sums to 1\n",
    "            visits /= np.sum(visits)\n",
    "\n",
    "            # Append state + value & policy targets to memory\n",
    "            self.append_to_memory(state, value, visits)\n",
    "\n",
    "            # If memory is full, perform a learning step\n",
    "            if self.memory_full:\n",
    "                self.learn()\n",
    "\n",
    "            # Perform action in game\n",
    "            state, _, done = self.game_engine.play(state, action)\n",
    "\n",
    "            # Flip the board\n",
    "            state = -state\n",
    "\n",
    "        # Increment total games played\n",
    "        self.total_games += 1\n",
    "\n",
    "        # Logging if verbose\n",
    "        if self.verbose:\n",
    "            print(\"\\rTotal Games:\", self.total_games, \"Items in Memory:\", self.current_memory_index, \"Search Iterations:\", self.search_iterations, end=\"\")\n",
    "\n",
    "    def append_to_memory(self, state, value, visits):\n",
    "        \"\"\"\n",
    "        Append state and MCTS results to memory buffers.\n",
    "        Args:\n",
    "            state (array-like): Current game state.\n",
    "            value (float): MCTS value for the game state.\n",
    "            visits (array-like): MCTS visit counts for available moves.\n",
    "        \"\"\"\n",
    "        # Calculate the encoded states\n",
    "        encoded_state = np.array(self.game_engine.encode_state(state))\n",
    "        encoded_state_augmented = np.array(self.game_engine.encode_state(state[:, ::-1]))\n",
    "\n",
    "        # Stack states and visits\n",
    "        states_stack = np.stack((encoded_state, encoded_state_augmented), axis=0)\n",
    "        visits_stack = np.stack((visits, visits[::-1]), axis=0)\n",
    "\n",
    "        # Convert the stacks to tensors\n",
    "        state_tensor = torch.tensor(states_stack, dtype=torch.float).to(self.model_config.device)\n",
    "        visits_tensor = torch.tensor(visits_stack, dtype=torch.float).to(self.model_config.device)\n",
    "        value_tensor = torch.tensor(np.array([value, value]), dtype=torch.float).to(self.model_config.device).unsqueeze(1)\n",
    "\n",
    "        # Store in pre-allocated GPU memory\n",
    "        self.state_memory[self.current_memory_index:self.current_memory_index + 2] = state_tensor\n",
    "        self.value_memory[self.current_memory_index:self.current_memory_index + 2] = value_tensor\n",
    "        self.policy_memory[self.current_memory_index:self.current_memory_index + 2] = visits_tensor\n",
    "\n",
    "        # Increment index, handle overflow\n",
    "        self.current_memory_index = (self.current_memory_index + 2) % self.max_memory\n",
    "\n",
    "        # Set memory filled flag to True if memory is full\n",
    "        if (self.current_memory_index == 0) or (self.current_memory_index == 1):\n",
    "            self.memory_full = True\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        \"Update the neural network by extracting minibatches from memory and performing one step of optimization for each one.\"\n",
    "        self.network.train()\n",
    "\n",
    "        # Create a randomly shuffled list of batch indices\n",
    "        batch_indices = np.arange(self.max_memory)\n",
    "        np.random.shuffle(batch_indices)\n",
    "\n",
    "        for batch_index in range(self.model_config.num_minibatches):\n",
    "            # Get minibatch indices\n",
    "            start = batch_index * self.model_config.minibatch_size\n",
    "            end = start + self.model_config.minibatch_size\n",
    "            mb_indices = batch_indices[start:end]\n",
    "\n",
    "            # Slice memory tensors\n",
    "            mb_states = self.state_memory[mb_indices]\n",
    "            mb_value_targets = self.value_memory[mb_indices]\n",
    "            mb_policy_targets = self.policy_memory[mb_indices]\n",
    "\n",
    "            # Network predictions\n",
    "            value_preds, policy_logits = self.network(mb_states)\n",
    "\n",
    "            # Loss calculation\n",
    "            policy_loss = self.loss_cross_entropy(policy_logits, mb_policy_targets)\n",
    "            value_loss = self.loss_mse(value_preds.view(-1), mb_value_targets.view(-1))\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.play()\n",
    "\n",
    "        self.memory_full = False\n",
    "        self.network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"Class to evaluate the policy network's performance on simple moves.\"\n",
    "    def __init__(self, alpha_mcts, num_examples=500, verbose=True):\n",
    "        self.network = alpha_mcts.network\n",
    "        self.game = alpha_mcts.game_engine\n",
    "        self.model_config = alpha_mcts.model_config\n",
    "        self.accuracies = []\n",
    "        self.num_examples = num_examples\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Generate and prepare example states and actions for evaluation\n",
    "        self.generate_examples()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"Select an action based on the given state, will choose winning or blocking moves.\"\n",
    "        valid_actions = self.game.get_valid_actions(state)\n",
    "        \n",
    "        # Check for a winning move\n",
    "        for action in valid_actions:\n",
    "            next_state, reward, _ = self.game.play(state, action)\n",
    "            if reward == 1:\n",
    "                return action\n",
    "\n",
    "        # Check for a blocking move\n",
    "        flipped_state = -state\n",
    "        for action in valid_actions:\n",
    "            next_state, reward, _ = self.game.play(flipped_state, action)\n",
    "            if reward == 1:\n",
    "                return action\n",
    "\n",
    "        # Default to random action if no winning or blocking move\n",
    "        return random.choice(valid_actions)\n",
    "\n",
    "    def generate_examples(self):\n",
    "        \"Generate and prepare example states and actions for evaluation.\"\n",
    "        winning_examples = self.generate_examples_for_condition('win')\n",
    "        blocking_examples = self.generate_examples_for_condition('block')\n",
    "\n",
    "        # Prepare states and actions for evaluation\n",
    "        winning_example_states, winning_example_actions = zip(*winning_examples)\n",
    "        blocking_example_states, blocking_example_actions = zip(*blocking_examples)\n",
    "\n",
    "        target_states = np.concatenate([winning_example_states, blocking_example_states], axis=0)\n",
    "        target_actions = np.concatenate([winning_example_actions, blocking_example_actions], axis=0)\n",
    "\n",
    "        encoded_states = [self.game.encode_state(state) for state in target_states]\n",
    "        self.X_target = torch.tensor(np.stack(encoded_states, axis=0), dtype=torch.float).to(self.model_config.device)\n",
    "        self.y_target = torch.tensor(target_actions, dtype=torch.long).to(self.model_config.device)\n",
    "\n",
    "    def generate_examples_for_condition(self, condition):\n",
    "        \"Generate examples based on either 'win' or 'block' conditions.\"\n",
    "        examples = []\n",
    "        while len(examples) < self.num_examples:\n",
    "            state = self.game.reset()\n",
    "            while True:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done = self.game.play(state, action, to_play=1)\n",
    "                \n",
    "                if condition == 'win' and reward == 1:\n",
    "                    examples.append((state, action))\n",
    "                    break\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "                # Flipping the board for opponent's perspective\n",
    "                action = self.select_action(-state)\n",
    "                next_state, reward, done = self.game.play(state, action, to_play=-1)\n",
    "                \n",
    "                if condition == 'block' and reward == -1:\n",
    "                    examples.append((-state, action))\n",
    "                    break\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "        return examples\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"Evaluate the policy network's accuracy and append it to self.accuracies.\"\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            _, logits = self.network(self.X_target)\n",
    "            pred_actions = logits.argmax(dim=1)\n",
    "            accuracy = (pred_actions == self.y_target).float().mean().item()\n",
    "        \n",
    "        self.accuracies.append(accuracy)\n",
    "        if self.verbose:\n",
    "            print(f\"Initial Evaluation Accuracy: {100 * accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_engine = Connect4Engine()\n",
    "# alpha_mcts = AlphaMCTS(game_engine, config)\n",
    "# evaluator = Evaluator(alpha_mcts)\n",
    "\n",
    "# # Evaluate pre-training\n",
    "# evaluator.evaluate()\n",
    "\n",
    "# # Main training/evaluation loop\n",
    "# for _ in range(config.training_epochs):\n",
    "#     alpha_mcts.train(1)\n",
    "#     evaluator.evaluate()\n",
    "\n",
    "# # Save trained weights\n",
    "# torch.save(alpha_mcts.network.state_dict(), 'alpha_mcts-network-weights-new.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot data\n",
    "# x_values = np.linspace(0, 101 * len(evaluator.accuracies), len(evaluator.accuracies))\n",
    "# y_values = [acc * 100 for acc in evaluator.accuracies]\n",
    "\n",
    "# # Create plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(x_values, y_values, linewidth=2, marker='o', markersize=4, linestyle='-', color='#636EFA')\n",
    "\n",
    "# # Formatting\n",
    "# plt.xlabel('\\nNumber of Games', fontsize=16)\n",
    "# plt.ylabel('Policy Evaluation Accuracy (%)', fontsize=16)\n",
    "# plt.title('Policy Evaluation\\n', fontsize=24)\n",
    "# plt.grid(True, linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Evaluation Accuracy: 85.1%\n"
     ]
    }
   ],
   "source": [
    "# Define the game, AlphaMCTS, and evaluator\n",
    "game_engine = Connect4Engine()\n",
    "alpha_mcts = AlphaMCTS(game_engine, config)\n",
    "evaluator = Evaluator(alpha_mcts)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "file_path = \"C:/Users/Rohan Arya/OneDrive/Desktop/FinalYearProject/alphamcts-network-weights-new.pth\"\n",
    "pre_trained_weights = torch.load(file_path, map_location=config.device)\n",
    "alpha_mcts.network.load_state_dict(pre_trained_weights)\n",
    "\n",
    "# Evaluate the pre-trained model\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaMCTSAgent:\n",
    "    def __init__(self, alpha_mcts):\n",
    "        self.alpha_mcts = alpha_mcts\n",
    "        self.alpha_mcts.network.eval()\n",
    "        \n",
    "        # Remove noise from move calculations\n",
    "        self.alpha_mcts.model_config.dirichlet_epsilon = 0\n",
    "\n",
    "    def select_action(self, state, search_iterations=1000):\n",
    "        state_tensor = torch.tensor(self.alpha_mcts.game_engine.encode_state(state), dtype=torch.float).to(self.alpha_mcts.model_config.device)\n",
    "        \n",
    "        # Get action without using search\n",
    "        if search_iterations == 0:\n",
    "            with torch.no_grad():\n",
    "                _, logits = self.alpha_mcts.network(state_tensor.unsqueeze(0))\n",
    "\n",
    "            # Get action probs and mask for valid actions\n",
    "            action_probs = F.softmax(logits.view(-1), dim=0).cpu().numpy()\n",
    "            valid_actions = self.alpha_mcts.game_engine.get_valid_actions(state)\n",
    "            valid_action_probs = action_probs[valid_actions]\n",
    "            best_action = valid_actions[np.argmax(valid_action_probs)]\n",
    "            return best_action\n",
    "        # Else use MCTS \n",
    "        else:\n",
    "            action, _ = self.alpha_mcts.mcts.search(state, search_iterations)\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI to play against agent using input box, choosing values from 0 to 6 for moves\n",
    "\n",
    "agent = AlphaMCTSAgent(alpha_mcts)\n",
    "# Constants for board dimensions\n",
    "BOARD_WIDTH = 7\n",
    "BOARD_HEIGHT = 6\n",
    "CELL_SIZE = 60\n",
    "DISK_RADIUS = CELL_SIZE // 2 - 5\n",
    "WINDOW_PADDING = 20\n",
    "\n",
    "# Function to handle human move\n",
    "def human_move():\n",
    "    global state, turn, done\n",
    "\n",
    "    # Get the action entered by the player\n",
    "    action = int(entry.get())\n",
    "\n",
    "    # Check if the action is valid\n",
    "    if action not in game_engine.get_valid_actions(state):\n",
    "        messagebox.showerror(\"Invalid Move\", \"Please enter a valid move.\")\n",
    "        return\n",
    "\n",
    "    # Perform the human move\n",
    "    next_state, reward, done = game_engine.play(state, action)\n",
    "    update_board(next_state)\n",
    "\n",
    "    # Check for game end\n",
    "    if done:\n",
    "        end_game(reward)\n",
    "    else:\n",
    "        state = -next_state\n",
    "        turn = 1 - turn\n",
    "        agent_move()\n",
    "\n",
    "# Function to handle agent move\n",
    "def agent_move():\n",
    "    global state, turn, done\n",
    "\n",
    "    # Let the agent select its action\n",
    "    action = agent.select_action(state, 1000)\n",
    "\n",
    "    # Perform the agent move\n",
    "    next_state, reward, done = game_engine.play(state, action)\n",
    "    update_board(next_state)\n",
    "\n",
    "    # Check for game end\n",
    "    if done:\n",
    "        end_game(reward)\n",
    "    else:\n",
    "        state = -next_state\n",
    "        turn = 1 - turn\n",
    "\n",
    "# Function to draw the board\n",
    "def draw_board():\n",
    "    for col in range(BOARD_WIDTH):\n",
    "        for row in range(BOARD_HEIGHT):\n",
    "            x0 = col * CELL_SIZE + WINDOW_PADDING\n",
    "            y0 = row * CELL_SIZE + WINDOW_PADDING\n",
    "            x1 = x0 + CELL_SIZE\n",
    "            y1 = y0 + CELL_SIZE\n",
    "            canvas.create_rectangle(x0, y0, x1, y1, fill=\"blue\", outline=\"black\")\n",
    "            canvas.create_oval(x0 + 5, y0 + 5, x1 - 5, y1 - 5, fill=\"white\", outline=\"black\")\n",
    "\n",
    "# Function to draw the disks\n",
    "def update_board(board):\n",
    "    canvas.delete(\"disk\")\n",
    "\n",
    "    for col in range(BOARD_WIDTH):\n",
    "        for row in range(BOARD_HEIGHT):\n",
    "            if board[row][col] == 1:\n",
    "                x_center = col * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                y_center = row * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                canvas.create_oval(x_center - DISK_RADIUS, y_center - DISK_RADIUS,\n",
    "                                   x_center + DISK_RADIUS, y_center + DISK_RADIUS,\n",
    "                                   fill=\"red\", outline=\"black\", tags=\"disk\")\n",
    "            elif board[row][col] == -1:\n",
    "                x_center = col * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                y_center = row * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                canvas.create_oval(x_center - DISK_RADIUS, y_center - DISK_RADIUS,\n",
    "                                   x_center + DISK_RADIUS, y_center + DISK_RADIUS,\n",
    "                                   fill=\"yellow\", outline=\"black\", tags=\"disk\")\n",
    "\n",
    "# Function to handle end of the game\n",
    "def end_game(reward):\n",
    "    if reward == -1:\n",
    "        messagebox.showinfo(\"Game Over\", \"You win!\")\n",
    "    elif reward == 1:\n",
    "        messagebox.showinfo(\"Game Over\", \"You lose!\")\n",
    "    else:\n",
    "        messagebox.showinfo(\"Game Over\", \"It's a draw!\")\n",
    "    root.quit()\n",
    "\n",
    "# Create the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Connect4 Game\")\n",
    "\n",
    "# Create a canvas to draw the board\n",
    "canvas = tk.Canvas(root, width=BOARD_WIDTH * CELL_SIZE + 2 * WINDOW_PADDING,\n",
    "                   height=BOARD_HEIGHT * CELL_SIZE + 2 * WINDOW_PADDING)\n",
    "canvas.pack()\n",
    "\n",
    "# Draw the board\n",
    "draw_board()\n",
    "\n",
    "# Create a label and entry widget for the player's move\n",
    "move_label = tk.Label(root, text=\"Enter your move (0-6):\")\n",
    "move_label.pack()\n",
    "entry = tk.Entry(root)\n",
    "entry.pack()\n",
    "\n",
    "# Create a button to submit the move\n",
    "submit_button = tk.Button(root, text=\"Submit\", command=human_move)\n",
    "submit_button.pack()\n",
    "\n",
    "# Initial setup\n",
    "state = game_engine.reset()\n",
    "turn = 0\n",
    "done = False\n",
    "update_board(state)\n",
    "\n",
    "# Start the main loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI where player can play against agent using cursor functionality\n",
    "agent = AlphaMCTSAgent(alpha_mcts)\n",
    "# Constants for board dimensions\n",
    "BOARD_WIDTH = 7\n",
    "BOARD_HEIGHT = 6\n",
    "CELL_SIZE = 60\n",
    "DISK_RADIUS = CELL_SIZE // 2 - 5\n",
    "WINDOW_PADDING = 20\n",
    "\n",
    "# Function to handle human move\n",
    "def human_move(event):\n",
    "    global state, turn, done\n",
    "\n",
    "    # Determine the column clicked\n",
    "    col = event.x // CELL_SIZE\n",
    "\n",
    "    # Check if the column is valid\n",
    "    if col < 0 or col >= BOARD_WIDTH:\n",
    "        return\n",
    "\n",
    "    # Check if the column is full\n",
    "    if all(state[row][col] != 0 for row in range(BOARD_HEIGHT)):\n",
    "        return\n",
    "\n",
    "    # Perform the human move\n",
    "    action = col\n",
    "    next_state, reward, done = game_engine.play(state, action)\n",
    "    update_board(next_state)\n",
    "\n",
    "    # Check for game end\n",
    "    if done:\n",
    "        end_game(reward)\n",
    "    else:\n",
    "        state = -next_state\n",
    "        turn = 1 - turn\n",
    "        agent_move()\n",
    "\n",
    "# Function to handle agent move\n",
    "def agent_move():\n",
    "    global state, turn, done\n",
    "\n",
    "    # Let the agent select its action\n",
    "    action = agent.select_action(state, 1000)\n",
    "\n",
    "    # Perform the agent move\n",
    "    next_state, reward, done = game_engine.play(state, action)\n",
    "    update_board(next_state)\n",
    "\n",
    "    # Check for game end\n",
    "    if done:\n",
    "        end_game(reward)\n",
    "    else:\n",
    "        state = -next_state\n",
    "        turn = 1 - turn\n",
    "\n",
    "# Function to draw the board\n",
    "def draw_board():\n",
    "    for col in range(BOARD_WIDTH):\n",
    "        for row in range(BOARD_HEIGHT):\n",
    "            x0 = col * CELL_SIZE + WINDOW_PADDING\n",
    "            y0 = row * CELL_SIZE + WINDOW_PADDING\n",
    "            x1 = x0 + CELL_SIZE\n",
    "            y1 = y0 + CELL_SIZE\n",
    "            canvas.create_rectangle(x0, y0, x1, y1, fill=\"blue\", outline=\"black\")\n",
    "            canvas.create_oval(x0 + 5, y0 + 5, x1 - 5, y1 - 5, fill=\"white\", outline=\"black\")\n",
    "\n",
    "# Function to draw the disks\n",
    "def update_board(board):\n",
    "    canvas.delete(\"disk\")\n",
    "\n",
    "    for col in range(BOARD_WIDTH):\n",
    "        for row in range(BOARD_HEIGHT):\n",
    "            if board[row][col] == 1:\n",
    "                x_center = col * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                y_center = row * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                canvas.create_oval(x_center - DISK_RADIUS, y_center - DISK_RADIUS,\n",
    "                                   x_center + DISK_RADIUS, y_center + DISK_RADIUS,\n",
    "                                   fill=\"red\", outline=\"black\", tags=\"disk\")\n",
    "            elif board[row][col] == -1:\n",
    "                x_center = col * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                y_center = row * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                canvas.create_oval(x_center - DISK_RADIUS, y_center - DISK_RADIUS,\n",
    "                                   x_center + DISK_RADIUS, y_center + DISK_RADIUS,\n",
    "                                   fill=\"yellow\", outline=\"black\", tags=\"disk\")\n",
    "\n",
    "# Function to handle end of the game\n",
    "def end_game(reward):\n",
    "    if reward == -1:\n",
    "        messagebox.showinfo(\"Game Over\", \"You win!\")\n",
    "    elif reward == 1:\n",
    "        messagebox.showinfo(\"Game Over\", \"You lose!\")\n",
    "    else:\n",
    "        messagebox.showinfo(\"Game Over\", \"It's a draw!\")\n",
    "    root.quit()\n",
    "\n",
    "# Create the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Connect4 Game\")\n",
    "\n",
    "# Create a canvas to draw the board\n",
    "canvas = tk.Canvas(root, width=BOARD_WIDTH * CELL_SIZE + 2 * WINDOW_PADDING,\n",
    "                   height=BOARD_HEIGHT * CELL_SIZE + 2 * WINDOW_PADDING)\n",
    "canvas.pack()\n",
    "\n",
    "# Draw the board\n",
    "draw_board()\n",
    "\n",
    "# Bind mouse click event to human move function\n",
    "canvas.bind(\"<Button-1>\", human_move)\n",
    "\n",
    "# Initial setup\n",
    "state = game_engine.reset()\n",
    "turn = 0\n",
    "done = False\n",
    "update_board(state)\n",
    "\n",
    "# Start the main loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# GUI to watch game between AlphaMCTS agent and Minimax algorithm\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import numpy as np\n",
    "\n",
    "agent = AlphaMCTSAgent(alpha_mcts)\n",
    "# Constants for board dimensions\n",
    "BOARD_WIDTH = 7\n",
    "BOARD_HEIGHT = 6\n",
    "CELL_SIZE = 60\n",
    "DISK_RADIUS = CELL_SIZE // 2 - 5\n",
    "WINDOW_PADDING = 20\n",
    "\n",
    "# Function to handle AlphaZero agent move\n",
    "def alpha_zero_move():\n",
    "    global state, turn, done\n",
    "\n",
    "    # Let the AlphaZero agent select its action\n",
    "    action = agent.select_action(state, 1000)\n",
    "\n",
    "    # Perform the agent move\n",
    "    next_state, reward, done = game_engine.play(state, action)\n",
    "    update_board(next_state)\n",
    "\n",
    "    # Check for game end\n",
    "    if done:\n",
    "        end_game(reward)\n",
    "    else:\n",
    "        state = -next_state\n",
    "        turn = 1 - turn\n",
    "\n",
    "# Function to handle Minimax agent move\n",
    "def minimax_move():\n",
    "    global state, turn, done\n",
    "\n",
    "    # Let the Minimax algorithm select its action\n",
    "    action = minimax(state, 3, float('-inf'), float('inf'), True)[1]\n",
    "\n",
    "    # Perform the agent move\n",
    "    next_state, reward, done = game_engine.play(state, action)\n",
    "    update_board(next_state)\n",
    "\n",
    "    # Check for game end\n",
    "    if done:\n",
    "        end_game(reward)\n",
    "    else:\n",
    "        state = -next_state\n",
    "        turn = 1 - turn\n",
    "        alpha_zero_move()\n",
    "\n",
    "# Minimax algorithm implementation\n",
    "def minimax(state, depth, alpha, beta, maximizing_player):\n",
    "    if depth == 0 or len(game_engine.get_valid_actions(state)) == 0:\n",
    "        return game_engine.evaluate(state), None\n",
    "\n",
    "    valid_actions = game_engine.get_valid_actions(state)\n",
    "\n",
    "    if maximizing_player:\n",
    "        value = float('-inf')\n",
    "        best_action = None\n",
    "        for action in valid_actions:\n",
    "            next_state, _, _ = game_engine.play(state, action)\n",
    "            eval, _ = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "            if eval > value:\n",
    "                value = eval\n",
    "                best_action = action\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta:\n",
    "                break  # Beta cut-off\n",
    "        return value, best_action\n",
    "    else:\n",
    "        value = float('inf')\n",
    "        best_action = None\n",
    "        for action in valid_actions:\n",
    "            next_state, _, _ = game_engine.play(state, action)\n",
    "            eval, _ = minimax(next_state, depth - 1, alpha, beta, True)\n",
    "            if eval < value:\n",
    "                value = eval\n",
    "                best_action = action\n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break  # Alpha cut-off\n",
    "        return value, best_action\n",
    "\n",
    "# Function to draw the board\n",
    "def draw_board():\n",
    "    for col in range(BOARD_WIDTH):\n",
    "        for row in range(BOARD_HEIGHT):\n",
    "            x0 = col * CELL_SIZE + WINDOW_PADDING\n",
    "            y0 = row * CELL_SIZE + WINDOW_PADDING\n",
    "            x1 = x0 + CELL_SIZE\n",
    "            y1 = y0 + CELL_SIZE\n",
    "            canvas.create_rectangle(x0, y0, x1, y1, fill=\"blue\", outline=\"black\")\n",
    "            canvas.create_oval(x0 + 5, y0 + 5, x1 - 5, y1 - 5, fill=\"white\", outline=\"black\")\n",
    "\n",
    "# Function to draw the disks\n",
    "def update_board(board):\n",
    "    canvas.delete(\"disk\")\n",
    "\n",
    "    for col in range(BOARD_WIDTH):\n",
    "        for row in range(BOARD_HEIGHT):\n",
    "            if board[row][col] == 1:\n",
    "                x_center = col * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                y_center = row * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                canvas.create_oval(x_center - DISK_RADIUS, y_center - DISK_RADIUS,\n",
    "                                   x_center + DISK_RADIUS, y_center + DISK_RADIUS,\n",
    "                                   fill=\"red\", outline=\"black\", tags=\"disk\")\n",
    "            elif board[row][col] == -1:\n",
    "                x_center = col * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                y_center = row * CELL_SIZE + CELL_SIZE // 2 + WINDOW_PADDING\n",
    "                canvas.create_oval(x_center - DISK_RADIUS, y_center - DISK_RADIUS,\n",
    "                                   x_center + DISK_RADIUS, y_center + DISK_RADIUS,\n",
    "                                   fill=\"yellow\", outline=\"black\", tags=\"disk\")\n",
    "\n",
    "# Function to handle end of the game\n",
    "def end_game(reward):\n",
    "    red_wins = reward == 1\n",
    "    yellow_wins = reward == -1\n",
    "\n",
    "    if red_wins:\n",
    "        messagebox.showinfo(\"Game Over\", \"AlphaZero wins!\")\n",
    "    elif yellow_wins:\n",
    "        messagebox.showinfo(\"Game Over\", \"Minimax loses!\")\n",
    "    else:\n",
    "        messagebox.showinfo(\"Game Over\", \"It's a draw!\")\n",
    "    root.quit()\n",
    "\n",
    "# Create the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Connect4 Game\")\n",
    "\n",
    "# Create a canvas to draw the board\n",
    "canvas = tk.Canvas(root, width=BOARD_WIDTH * CELL_SIZE + 2 * WINDOW_PADDING,\n",
    "                   height=BOARD_HEIGHT * CELL_SIZE + 2 * WINDOW_PADDING)\n",
    "canvas.pack()\n",
    "\n",
    "# Draw the board\n",
    "draw_board()\n",
    "\n",
    "# Initial setup\n",
    "state = game_engine.reset()\n",
    "turn = 0\n",
    "done = False\n",
    "update_board(state)     \n",
    "\n",
    "# Start the game loop: AlphaZero vs Minimax\n",
    "def play_game():\n",
    "    global state, turn, done\n",
    "\n",
    "    if done:\n",
    "        return\n",
    "\n",
    "    if turn == 0:\n",
    "        alpha_zero_move()\n",
    "    else:\n",
    "        minimax_move()\n",
    "\n",
    "    # Check for game end\n",
    "    if done:\n",
    "        if turn == 0:\n",
    "            end_game(-1)  # Minimax wins\n",
    "        elif turn == 1:\n",
    "            end_game(1)  # AlphaZero wins\n",
    "        else:\n",
    "            end_game(0)  # Draw\n",
    "    else:\n",
    "        root.after(1000, play_game)  # Schedule the next move after 1 second\n",
    "\n",
    "# Start the game loop\n",
    "root.after(1000, play_game)  # Start the game loop after 1 second\n",
    "\n",
    "# Start the main loop\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
